# Creating a quick local RAG to help you find information in your documents



## Setup LLM

For this quick RAG we'll use Ollama which is a robust LLM server taht can be set up locally. 

Setting up and running Ollama is straightforward. 

First, go to ollama.ai and download the app. 

Next, open terminal, and execute the following command to pull that latest Mistral model. 

ollama pull mistral

You can use this model with the following command. 

ollama run mistral